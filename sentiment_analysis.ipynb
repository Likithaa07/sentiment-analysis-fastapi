{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of Amazon Customer Reviews\n",
        "\n",
        "The aim of this project is to develop a machine learning pipeline for sentiment analysis on Amazon customer reviews using natural language processing techniques. By analyzing review texts, the project seeks to automatically classify the sentiment expressed in each review as positive or negative, providing valuable insights into overall customer satisfaction and opinions."
      ],
      "metadata": {
        "id": "B5fTuj1wwZ5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data Loading and Initial Exploration\n",
        "\n",
        "Load the raw Amazon customer reviews dataset and perform a basic inspection to understand its structure and contents. This step establishes a foundation for further data cleaning and analysis.\n"
      ],
      "metadata": {
        "id": "ZKJkbeYHw8KN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6uqXN--DInJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/test.ft.txt', sep='\\t', header=None, names=['review'])\n",
        "\n",
        "# Display first five rows and dataset shape\n",
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Label Extraction and Data Cleaning\n",
        "\n",
        "Extract sentiment labels from the review text and clean reviews by removing label tags and extra spaces to prepare for analysis."
      ],
      "metadata": {
        "id": "CmnaP7F0xUvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sentiment labels\n",
        "df['label'] = df['review'].apply(lambda x: 1 if '__label__1' in x else 2)\n",
        "\n",
        "# Remove label tags from review text\n",
        "df['clean_review'] = df['review'].apply(lambda x: x.replace('__label__1', '').replace('__label__2', '').strip())\n",
        "\n",
        "# Display cleaned data sample\n",
        "print(df[['label', 'clean_review']].head())"
      ],
      "metadata": {
        "id": "Ltc0z99FxIhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Text Preprocessing\n",
        "\n",
        "Apply text preprocessing techniques including lowercasing, removal of punctuation, and tokenization to prepare the review text for feature extraction and modeling.\n"
      ],
      "metadata": {
        "id": "G6V3q4lix3E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "A5OnHkOuytuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "JtuLSz6vyxme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "sHx2lbw2hPWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')  # Download the new resource instead of 'punkt'\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)  # This now uses punkt_tab internally\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['processed_review'] = df['clean_review'].apply(preprocess_text)\n",
        "\n",
        "print(df[['processed_review']].head())"
      ],
      "metadata": {
        "id": "gJNxU0wHyZNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Feature Extraction using TF-IDF Vectorization\n",
        "\n",
        "Convert the preprocessed text reviews into numerical features using TF-IDF vectorization to represent the importance of words in each review for model training.\n"
      ],
      "metadata": {
        "id": "4jj-pk95zhs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer with common parameters\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fit and transform the processed reviews to create feature vectors\n",
        "X = tfidf_vectorizer.fit_transform(df['processed_review'])\n",
        "\n",
        "# Display the shape of the feature matrix\n",
        "print(f\"Feature matrix shape: {X.shape}\")"
      ],
      "metadata": {
        "id": "fyHeLG2szHkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Model Training and Evaluation\n",
        "\n",
        "Split the feature matrix and labels into training and testing datasets. Train a machine learning classifier on the training set and evaluate its performance on the test set.\n"
      ],
      "metadata": {
        "id": "-BR-WiORz0_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "6Tj4LnpJzm2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Model Saving and Loading\n",
        "\n",
        "Save the trained sentiment analysis model and TF-IDF vectorizer to disk for future use, and demonstrate how to load them back for inference.\n"
      ],
      "metadata": {
        "id": "kYNk7HO70J1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model and vectorizer\n",
        "joblib.dump(model, 'sentiment_model.pkl')\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "# Load the model and vectorizer (example)\n",
        "loaded_model = joblib.load('sentiment_model.pkl')\n",
        "loaded_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "# Example inference on new text\n",
        "sample_text = \"This product exceeded expectations!\"\n",
        "\n",
        "# Preprocess sample text (use the same preprocessing function defined earlier)\n",
        "processed_text = preprocess_text(sample_text)\n",
        "\n",
        "# Vectorize the processed sample text\n",
        "sample_features = loaded_vectorizer.transform([processed_text])\n",
        "\n",
        "# Predict sentiment label\n",
        "prediction = loaded_model.predict(sample_features)\n",
        "\n",
        "print(f\"Predicted Sentiment Label: {prediction[0]}\")"
      ],
      "metadata": {
        "id": "MvvFYlDa0BN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model predicted the sentiment label **1** for the sample review \"This product exceeded expectations!\".\n",
        "\n",
        "Typically, in this sentiment analysis setup:\n",
        "- **Label 1** indicates **negative** sentiment.\n",
        "- **Label 2** indicates **positive** sentiment.\n",
        "\n",
        "This result suggests the model classified the sample review as negative. It is advisable to verify the label-to-sentiment mapping to ensure consistency."
      ],
      "metadata": {
        "id": "HREN6p2Z0id8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Model Deployment and Inference Function Creation\n",
        "\n",
        "Create a reusable inference function that loads the saved model and vectorizer, preprocesses new input text, and returns the predicted sentiment label for easy deployment integration.\n"
      ],
      "metadata": {
        "id": "qfwh44UJ0-vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load saved model and vectorizer\n",
        "loaded_model = joblib.load('sentiment_model.pkl')\n",
        "loaded_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    # Preprocess the input text (use the same preprocessing function defined earlier)\n",
        "    processed_text = preprocess_text(text)\n",
        "    # Vectorize the processed text\n",
        "    features = loaded_vectorizer.transform([processed_text])\n",
        "    # Predict and return the sentiment label\n",
        "    prediction = loaded_model.predict(features)\n",
        "    return prediction[0]\n",
        "\n",
        "# Example usage\n",
        "sample_input = \"The product quality was outstanding and delivery was quick.\"\n",
        "predicted_label = predict_sentiment(sample_input)\n",
        "print(f\"Predicted Sentiment Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "oe5dya__0Pj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inference function created in Step 7 predicts sentiment labels for new texts. The output \"Predicted Sentiment Label: 2\" indicates the model identified the input as positive sentiment.  "
      ],
      "metadata": {
        "id": "E-2oqDcQ1wKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_labeled_texts(filepath):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # Example line: \"__label__pos Some positive review text here.\"\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Split label and text\n",
        "            # Assuming FastText style: label starts with __label__ followed by space-separated text\n",
        "            parts = line.split(' ', 1)\n",
        "            if len(parts) != 2:\n",
        "                continue  # skip malformed lines\n",
        "\n",
        "            label_part, text_part = parts\n",
        "            label = label_part.replace('__label__', '')\n",
        "            texts.append(text_part)\n",
        "            labels.append(label)\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "8nLtuNDlw3h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from the original file\n",
        "raw_texts, labels = load_labeled_texts('/content/test.ft.txt')\n",
        "\n",
        "# Now you can proceed with raw_texts and labels as needed\n",
        "print(f\"Loaded {len(raw_texts)} samples.\")"
      ],
      "metadata": {
        "id": "cyOE1bQzjtdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_original_dataset(input_path='/content/test.ft.txt', output_path='/content/new_data.txt'):\n",
        "    import re\n",
        "\n",
        "    label_pattern = re.compile(r'^__label__(\\d+)\\s+(.*)')  # extract label and text\n",
        "    cleaned_lines = []\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as infile:\n",
        "        for line_no, line in enumerate(infile, start=1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            match = label_pattern.match(line)\n",
        "            if not match:\n",
        "                print(f\"Skipping line {line_no} with unexpected format.\")\n",
        "                continue\n",
        "\n",
        "            label, text = match.groups()\n",
        "            # Basic cleaning: lowercase and strip\n",
        "            cleaned_text = text.lower().strip()\n",
        "\n",
        "            cleaned_lines.append(f\"{label}\\t{cleaned_text}\")\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "        for cline in cleaned_lines:\n",
        "            outfile.write(cline + '\\n')\n",
        "\n",
        "    print(f\"Original data cleaned and saved to: {output_path}\")\n",
        "    print(f\"Total cleaned lines: {len(cleaned_lines)}\")\n",
        "\n",
        "# Run the cleaning\n",
        "clean_original_dataset()"
      ],
      "metadata": {
        "id": "dWPDaKXFlIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_head(file_path, n=5):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= n:\n",
        "                break\n",
        "            print(line.strip())\n",
        "\n",
        "# Print the first 5 lines of the cleaned data\n",
        "print_head('/content/new_data.txt', n=5)"
      ],
      "metadata": {
        "id": "PTX5l41xlPX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = '/content/new_data.txt'\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    size = os.path.getsize(file_path)\n",
        "    print(f\"File '{file_path}' exists and is {size} bytes\")\n",
        "    if size > 0:\n",
        "        # Print first few lines\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for _ in range(5):\n",
        "                print(f.readline().strip())\n",
        "    else:\n",
        "        print(f\"File '{file_path}' is empty.\")\n",
        "else:\n",
        "    print(f\"File '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "id": "d-xN0n_Ml2U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/new_data.txt', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(line.strip())\n",
        "        if i >= 9:  # print first 10 lines\n",
        "            break"
      ],
      "metadata": {
        "id": "eU-0KMyLxNeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/new_data.txt"
      ],
      "metadata": {
        "id": "3iqypW4_xRvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/new_data.txt\n",
        "with open('/content/new_data.txt', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(line.strip())\n",
        "        if i >= 9:\n",
        "            break"
      ],
      "metadata": {
        "id": "OIFJeJiG0B-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Model Evaluation on New Data and Reporting\n",
        "\n",
        "In this step, you evaluate the deployed model's performance on a new, unseen dataset (or a hold-out validation set) to verify its generalization ability. You also generate a detailed evaluation report.\n"
      ],
      "metadata": {
        "id": "fmMHLtE-21S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_labeled_texts(filepath):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # The first character is label (1 or 2), followed by text without a space\n",
        "            label_char = line[0]\n",
        "            text = line[1:].strip()\n",
        "\n",
        "            if label_char not in ('1', '2') or not text:\n",
        "                continue  # skip malformed lines\n",
        "\n",
        "            labels.append(label_char)\n",
        "            texts.append(text)\n",
        "    if not texts:\n",
        "        raise ValueError(f\"No valid labeled data found in '{filepath}'\")\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "6r-jkjnOyAeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_texts, labels = load_labeled_texts('/content/new_data.txt')\n",
        "print(f\"Loaded {len(raw_texts)} samples.\")\n",
        "print(f\"Example label/text: {labels[0]} / {raw_texts[0][:100]} ...\")"
      ],
      "metadata": {
        "id": "CWQv8xJlyDyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Paths to your files\n",
        "dataset_path = '/content/new_data.txt'\n",
        "model_path = 'sentiment_model.pkl'       # adjust if your model filename differs\n",
        "vectorizer_path = 'tfidf_vectorizer.pkl' # adjust if your vectorizer filename differs\n",
        "\n",
        "# Correct loader for your data format (label is first char, then text)\n",
        "def load_labeled_texts(file_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_no, line in enumerate(f, start=1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            label_char = line[0]\n",
        "            text = line[1:].strip()\n",
        "\n",
        "            if label_char not in ('1', '2'):\n",
        "                print(f\"Warning: Invalid label '{label_char}' at line {line_no}. Skipping this line.\")\n",
        "                continue\n",
        "\n",
        "            if not text:\n",
        "                print(f\"Warning: Empty text at line {line_no}. Skipping this line.\")\n",
        "                continue\n",
        "\n",
        "            labels.append(int(label_char))\n",
        "            texts.append(text)\n",
        "\n",
        "    if not texts:\n",
        "        raise ValueError(f\"No valid labeled data found in '{file_path}'.\")\n",
        "    return texts, labels\n",
        "\n",
        "# Load your data\n",
        "new_raw_texts, new_labels = load_labeled_texts(dataset_path)\n",
        "print(f\"Loaded {len(new_raw_texts)} samples.\")\n",
        "print(f\"Example label/text: {new_labels[0]} / {new_raw_texts[0][:100]} ...\")\n",
        "\n",
        "# Define your preprocessing function\n",
        "def preprocess_text(text):\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Preprocess all texts\n",
        "new_X_processed = [preprocess_text(text) for text in new_raw_texts]\n",
        "\n",
        "# Load saved vectorizer and model\n",
        "loaded_vectorizer = joblib.load(vectorizer_path)\n",
        "loaded_model = joblib.load(model_path)\n",
        "\n",
        "# Transform texts into features using vectorizer\n",
        "new_features = loaded_vectorizer.transform(new_X_processed)\n",
        "\n",
        "# Predict labels on new data\n",
        "new_y_pred = loaded_model.predict(new_features)\n",
        "\n",
        "# Print classification report comparing true vs predicted labels\n",
        "print(\"Classification Report on New Data:\")\n",
        "print(classification_report(new_labels, new_y_pred))\n",
        "\n",
        "# Plot confusion matrix for deeper insight\n",
        "cm = confusion_matrix(new_labels, new_y_pred)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative (1)', 'Positive (2)'],\n",
        "            yticklabels=['Negative (1)', 'Positive (2)'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.title('Confusion Matrix on New Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G6N7PxcT1Xsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification report shows perfect model performance with 100% precision, recall, and F1-score on the new data samples.  \n",
        "All 3 test instances (1 negative, 2 positive) were correctly classified, indicating excellent accuracy on this small dataset."
      ],
      "metadata": {
        "id": "quJCiHd-Qxsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Analyze Errors & Save Model Pipeline\n",
        "\n",
        "- Review misclassified samples to understand model errors.  \n",
        "- Save the combined vectorizer and model as a pipeline for easy reuse.\n"
      ],
      "metadata": {
        "id": "_7li65reRI9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/test.ft.txt /content/new_data.txt"
      ],
      "metadata": {
        "id": "vLyIgC1oVRWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filepath):\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(maxsplit=1)\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            label_str, text = parts[0], parts[1]\n",
        "\n",
        "            # If label starts with '__label__', trim and convert to int\n",
        "            if label_str.startswith('__label__'):\n",
        "                numeric_label_str = label_str.replace('__label__', '')\n",
        "                try:\n",
        "                    label = int(numeric_label_str)\n",
        "                except ValueError:\n",
        "                    label = label_str  # fallback to original if conversion fails\n",
        "            else:\n",
        "                # fallback if not prefixed label\n",
        "                try:\n",
        "                    label = int(label_str)\n",
        "                except ValueError:\n",
        "                    label = label_str\n",
        "\n",
        "            labels.append(label)\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "Z8_FdjOOVlIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = load_dataset('/content/test.ft.txt')\n",
        "X_new, y_new = load_dataset('/content/new_data.txt')\n",
        "\n",
        "print(f\"Sample test label: {y_test[0]}\")  # Should print 1 or 2, not '__label__1'\n",
        "print(f\"Sample test text: {X_test[0]}\")"
      ],
      "metadata": {
        "id": "ZCdpA8fWV6dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pipeline"
      ],
      "metadata": {
        "id": "5humIG9TW8w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install joblib"
      ],
      "metadata": {
        "id": "2saqPUynXFIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the sentiment analysis model pipeline\n",
        "pipeline = joblib.load('/content/sentiment_model.pkl')\n",
        "\n",
        "# Load the TF-IDF vectorizer separately, if needed\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "id": "fVD83wW6d-43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in /content to check presence\n",
        "!ls -l /content/\n",
        "\n",
        "# Then load models\n",
        "import joblib\n",
        "pipeline = joblib.load('/content/sentiment_model.pkl')\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "id": "8Cfrr_YIeCRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filepath):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(maxsplit=1)\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            label_str, text = parts[0], parts[1]\n",
        "\n",
        "            # Extract numeric label from strings like '__label__2'\n",
        "            if label_str.startswith('__label__'):\n",
        "                label_num_str = label_str.replace('__label__', '')\n",
        "                try:\n",
        "                    label = int(label_num_str)\n",
        "                except ValueError:\n",
        "                    label = label_str  # fallback to string if conversion fails\n",
        "            else:\n",
        "                # fallback to handle other unexpected label formats\n",
        "                try:\n",
        "                    label = int(label_str)\n",
        "                except ValueError:\n",
        "                    label = label_str\n",
        "\n",
        "            labels.append(label)\n",
        "            texts.append(text)\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "cR43vPDJf-7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the sentiment analysis model pipeline\n",
        "pipeline = joblib.load('/content/sentiment_model.pkl')\n",
        "\n",
        "# Load TF-IDF vectorizer if needed (optional depending on your pipeline)\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "id": "Mq3164UWerNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = load_dataset('/content/test.ft.txt')\n",
        "\n",
        "print(f\"Sample label: {y_test[0]}\")  # Should now print 1 or 2 (an integer)\n",
        "print(f\"Sample text: {X_test[0]}\")"
      ],
      "metadata": {
        "id": "_Hvts8tTfgC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "pipeline = joblib.load('/content/sentiment_model.pkl')"
      ],
      "metadata": {
        "id": "gC2iC552gLTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load vectorizer and model separately\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')\n",
        "sentiment_model = joblib.load('/content/sentiment_model.pkl')\n",
        "\n",
        "# Transform text data to feature vectors (2D array)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)  # This returns a sparse matrix\n",
        "\n",
        "# Predict with the model using transformed features\n",
        "y_pred = sentiment_model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "odluysfEgOac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Corrected load_dataset function to parse labels\n",
        "def load_dataset(filepath):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(maxsplit=1)\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            label_str, text = parts[0], parts[1]\n",
        "\n",
        "            if label_str.startswith('__label__'):\n",
        "                label_num_str = label_str.replace('__label__', '')\n",
        "                try:\n",
        "                    label = int(label_num_str)\n",
        "                except ValueError:\n",
        "                    label = label_str\n",
        "            else:\n",
        "                try:\n",
        "                    label = int(label_str)\n",
        "                except ValueError:\n",
        "                    label = label_str\n",
        "\n",
        "            labels.append(label)\n",
        "            texts.append(text)\n",
        "    return texts, labels\n",
        "\n",
        "# Load test dataset\n",
        "X_test, y_test = load_dataset('/content/test.ft.txt')\n",
        "\n",
        "# Load models\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')\n",
        "sentiment_model = joblib.load('/content/sentiment_model.pkl')\n",
        "\n",
        "# Transform raw test texts to TF-IDF feature vectors\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Predict labels\n",
        "y_pred = sentiment_model.predict(X_test_tfidf)\n",
        "\n",
        "# Error analysis\n",
        "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]\n",
        "\n",
        "print(f\"Total samples: {len(X_test)}\")\n",
        "print(f\"Number of misclassified samples: {len(misclassified_indices)}\")\n",
        "\n",
        "label_map = {1: \"Negative\", 2: \"Positive\"}\n",
        "\n",
        "print(\"\\nSome misclassified samples:\")\n",
        "for i in misclassified_indices[:5]:\n",
        "    print(f\"Sample index: {i}\")\n",
        "    print(f\"Text: {X_test[i]}\")\n",
        "    print(f\"True label: {label_map.get(y_test[i], y_test[i])}\")\n",
        "    print(f\"Predicted label: {label_map.get(y_pred[i], y_pred[i])}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "A9cRyuv-glcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "XsY4TLbIiTEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieved approximately 86% accuracy on a large 400,000-sample test set, demonstrating solid performance on a challenging sentiment classification task. Despite some misclassifications, the results indicate the model can reliably distinguish positive and negative sentiments in real-world reviews."
      ],
      "metadata": {
        "id": "V7w3P18uRttJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Model Deployment\n",
        "\n",
        "In this step, you deploy your saved sentiment analysis pipeline as a simple web API to serve predictions. Deployment makes your model accessible for real-time inference by other applications or users. This example uses Flask, a lightweight Python web framework, to create an endpoint that accepts text input and returns predicted sentiment."
      ],
      "metadata": {
        "id": "JlcJlqpPSV29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "# any other setup, such as route definitions, BEFORE calling app.run"
      ],
      "metadata": {
        "id": "xbzzIhEXaI9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Add more setup: loading vectorizer, model, route definitions...\n",
        "# Example:\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "sentiment_model = joblib.load('sentiment_model.pkl')\n",
        "label_map = {1: \"Negative\", 2: \"Positive\"}\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # prediction code...\n",
        "    pass\n",
        "\n",
        "# Now *after* all that:\n",
        "app.run(host='0.0.0.0', port=5001, use_reloader=False)"
      ],
      "metadata": {
        "id": "YYuvjxs1aMAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyngrok"
      ],
      "metadata": {
        "id": "9D01RDBZahed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"30DzDLzHbU7gaimgih8PqfUrQYI_89BsgqqKyie4bMtU8jhvS\")"
      ],
      "metadata": {
        "id": "p9ahzyFZbXWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()  # Cleans up any old tunnels\n",
        "public_url = ngrok.connect(5001)\n",
        "print(\"Ngrok URL:\", public_url)"
      ],
      "metadata": {
        "id": "zOEqIfW9acmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "uQJp67U0cUmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(5001)\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "ZwlYF_0ZcJAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "\n",
        "app = Flask(__name__)\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')\n",
        "sentiment_model = joblib.load('/content/sentiment_model.pkl')\n",
        "label_map = {1: \"Negative\", 2: \"Positive\"}\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    text = data.get('text', '')\n",
        "    if not text:\n",
        "        return jsonify({'error': 'No text provided'}), 400\n",
        "    features = tfidf_vectorizer.transform([text])\n",
        "    pred_label = sentiment_model.predict(features)[0]\n",
        "    pred_sentiment = label_map.get(pred_label, str(pred_label))\n",
        "    return jsonify({\n",
        "        'text': text,\n",
        "        'predicted_label': int(pred_label),\n",
        "        'predicted_sentiment': pred_sentiment\n",
        "    })"
      ],
      "metadata": {
        "id": "BxusOQqhkasY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"30DzDLzHbU7gaimgih8PqfUrQYI_89BsgqqKyie4bMtU8jhvS\")  # if not set already\n",
        "public_url = ngrok.connect(5000)\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "nLFGXFWsl1vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load your pre-trained TF-IDF vectorizer and sentiment model\n",
        "# Replace these paths with the actual locations of your files\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')\n",
        "sentiment_model = joblib.load('/content/sentiment_model.pkl')\n",
        "\n",
        "# Mapping from numeric label to sentiment text\n",
        "label_map = {1: \"Negative\", 2: \"Positive\"}\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "\n",
        "    text = data.get('text')\n",
        "    if not text:\n",
        "        return jsonify({'error': 'No text provided'}), 400\n",
        "\n",
        "    # Vectorize input text\n",
        "    features = tfidf_vectorizer.transform([text])\n",
        "\n",
        "    # Predict sentiment\n",
        "    pred_label = sentiment_model.predict(features)[0]\n",
        "    pred_sentiment = label_map.get(pred_label, \"Unknown\")\n",
        "\n",
        "    # Optionally get confidence if your model supports predict_proba\n",
        "    confidence = None\n",
        "    if hasattr(sentiment_model, 'predict_proba'):\n",
        "        proba = sentiment_model.predict_proba(features)[0]\n",
        "        confidence = max(proba)\n",
        "\n",
        "    response = {\n",
        "        'text': text,\n",
        "        'predicted_label': int(pred_label),\n",
        "        'predicted_sentiment': pred_sentiment,\n",
        "    }\n",
        "    if confidence is not None:\n",
        "        response['confidence'] = round(confidence, 3)  # rounded to 3 decimals\n",
        "\n",
        "    return jsonify(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run the Flask app on port 5000\n",
        "    app.run(host='0.0.0.0', port=5001, use_reloader=False)\n"
      ],
      "metadata": {
        "id": "qwuh0WEH0r8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Example texts to predict\n",
        "texts = [\n",
        "    \"I love this product! It works great.\",\n",
        "    \"This is the worst experience I have had.\",\n",
        "    \"It's okay, not great.\"\n",
        "]\n",
        "\n",
        "# Get predictions\n",
        "predictions = sentiment_pipeline(texts)\n",
        "\n",
        "for text, pred in zip(texts, predictions):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Sentiment: {pred['label']}, Confidence: {pred['score']:.3f}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "iqutGSgt1bEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "\n",
        "app = FastAPI()\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(request: TextRequest):\n",
        "    result = classifier(request.text)\n",
        "    return {\"label\": result[0]['label'], \"score\": result[0]['score']}"
      ],
      "metadata": {
        "id": "JUa_sPIAMjL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step11: Exposing Your FastAPI App to the Web with ngrok\n",
        "\n",
        "This step sets up ngrok in your Google Colab environment to create a public URL for your FastAPI server.  \n",
        "It allows anyone to access your sentiment analysis API from anywhere, making it easy to share your demo for testing or showcasing.\n"
      ],
      "metadata": {
        "id": "im6-UhZ4iML4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok transformers"
      ],
      "metadata": {
        "id": "qnqjOp57iQjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(request: TextRequest):\n",
        "    result = classifier(request.text)\n",
        "    return {\"label\": result[0]['label'], \"score\": result[0]['score']}\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"Welcome to the sentiment analysis API. Use /predict with POST JSON.\"}\n",
        "# Expose your local server to the web\n",
        "public_url = ngrok.connect(8000)\n",
        "print('Public URL:', public_url)\n",
        "\n",
        "# Run the app (non-blocking, for Colab)\n",
        "uvicorn.run(app, host='0.0.0.0', port=8000)\n",
        "\n",
        "# Expose your local server to the web\n",
        "public_url = ngrok.connect(8000)\n",
        "print('Public URL:', public_url)\n",
        "\n",
        "# Run the app (non-blocking, for Colab)\n",
        "uvicorn.run(app, host='0.0.0.0', port=8000)"
      ],
      "metadata": {
        "id": "exXDTce8iYh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FastAPI App Running with ngrok Public URL\n",
        "\n",
        "The FastAPI server is successfully running and exposed to the internet using ngrok.  \n",
        "Visiting the base URL returns a friendly JSON message confirming the API is live:  \n",
        "`{\"message\":\"Welcome to the sentiment analysis API. Use /predict with POST JSON.\"}`\n",
        "\n",
        "This indicates your backend is reachable, and you can now send POST requests to the `/predict` endpoint to get sentiment analysis results.  \n",
        "The ngrok tunnel provides a temporary public URL to share and demo your API easily."
      ],
      "metadata": {
        "id": "hhyRODKalyRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 12: Simple Front-End Webpage for Sentiment Analysis API\n",
        "\n",
        "This step creates a basic HTML and JavaScript webpage that lets users input text and sends it to your FastAPI `/predict` endpoint. It displays the sentiment label and confidence score directly on the page for easy interaction."
      ],
      "metadata": {
        "id": "WeZXcic8l4N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok"
      ],
      "metadata": {
        "id": "N8Z1KyLTmjfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.responses import HTMLResponse\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Enable nested event loops for Colab async compatibility\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load your sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# API endpoint for sentiment prediction\n",
        "@app.post(\"/predict\")\n",
        "def predict(request: TextRequest):\n",
        "    result = classifier(request.text)\n",
        "    return {\"label\": result[0]['label'], \"score\": result[0]['score']}\n",
        "\n",
        "# Serve your frontend HTML page\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\" />\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
        "    <title>Sentiment Analysis Demo</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 40px; max-width: 600px; }\n",
        "        textarea { width: 100%; height: 100px; font-size: 1rem; }\n",
        "        button { margin-top: 10px; padding: 8px 16px; font-size: 1rem; }\n",
        "        #result { margin-top: 20px; font-weight: bold; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h2>Sentiment Analysis</h2>\n",
        "    <textarea id=\"inputText\" placeholder=\"Type your text here...\"></textarea><br />\n",
        "    <button onclick=\"getSentiment()\">Analyze Sentiment</button>\n",
        "    <div id=\"result\"></div>\n",
        "<script>\n",
        "async function getSentiment() {\n",
        "    const text = document.getElementById('inputText').value.trim();\n",
        "    if (!text) {\n",
        "        alert('Please enter some text!');\n",
        "        return;\n",
        "    }\n",
        "    const resultDiv = document.getElementById('result');\n",
        "    resultDiv.textContent = \"Analyzing...\";\n",
        "    try {\n",
        "        const response = await fetch('/predict', {\n",
        "            method: 'POST',\n",
        "            headers: { 'Content-Type': 'application/json' },\n",
        "            body: JSON.stringify({ text })\n",
        "        });\n",
        "        if (!response.ok) {\n",
        "            throw new Error('Error from API: ' + response.status);\n",
        "        }\n",
        "        const data = await response.json();\n",
        "        resultDiv.textContent = `Sentiment: ${data.label} (Confidence: ${(data.score * 100).toFixed(2)}%)`;\n",
        "    } catch (error) {\n",
        "        resultDiv.textContent = 'Error: ' + error.message;\n",
        "    }\n",
        "}\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "def root():\n",
        "    return html_content\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run Uvicorn in a separate thread so Colab does not block\n",
        "def run():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "thread = threading.Thread(target=run)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "G5pv9uYcm4LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = \"\"\"\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import HTMLResponse\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Enable nested event loops for Colab async compatibility\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load your sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# API endpoint for sentiment prediction\n",
        "@app.post(\"/predict\")\n",
        "def predict(request: TextRequest):\n",
        "    result = classifier(request.text)\n",
        "    return {\"label\": result[0]['label'], \"score\": result[0]['score']}\n",
        "\n",
        "# Serve your frontend HTML page\n",
        "html_content = \\\"\\\"\\\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\" />\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
        "    <title>Sentiment Analysis Demo</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 40px; max-width: 600px; }\n",
        "        textarea { width: 100%; height: 100px; font-size: 1rem; }\n",
        "        button { margin-top: 10px; padding: 8px 16px; font-size: 1rem; }\n",
        "        #result { margin-top: 20px; font-weight: bold; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h2>Sentiment Analysis</h2>\n",
        "    <textarea id=\"inputText\" placeholder=\"Type your text here...\"></textarea><br />\n",
        "    <button onclick=\"getSentiment()\">Analyze Sentiment</button>\n",
        "    <div id=\"result\"></div>\n",
        "<script>\n",
        "async function getSentiment() {\n",
        "    const text = document.getElementById('inputText').value.trim();\n",
        "    if (!text) {\n",
        "        alert('Please enter some text!');\n",
        "        return;\n",
        "    }\n",
        "    const resultDiv = document.getElementById('result');\n",
        "    resultDiv.textContent = \"Analyzing...\";\n",
        "    try {\n",
        "        const response = await fetch('/predict', {\n",
        "            method: 'POST',\n",
        "            headers: { 'Content-Type': 'application/json' },\n",
        "            body: JSON.stringify({ text })\n",
        "        });\n",
        "        if (!response.ok) {\n",
        "            throw new Error('Error from API: ' + response.status);\n",
        "        }\n",
        "        const data = await response.json();\n",
        "        resultDiv.textContent = `Sentiment: ${data.label} (Confidence: ${(data.score * 100).toFixed(2)}%)`;\n",
        "    } catch (error) {\n",
        "        resultDiv.textContent = 'Error: ' + error.message;\n",
        "    }\n",
        "}\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "def root():\n",
        "    return html_content\n",
        "\n",
        "# Start ngrok tunnel to expose port 8000\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run Uvicorn server in a separate thread so it doesn't block Colab\n",
        "def run():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "thread = threading.Thread(target=run)\n",
        "thread.start()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"main.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"File 'main.py' has been saved!\")"
      ],
      "metadata": {
        "id": "KvsMlflRqhnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import conf\n",
        "\n",
        "# Replace 'YOUR_NGROK_AUTHTOKEN' with your actual token from ngrok dashboard\n",
        "conf.get_default().auth_token = \"30DzDLzHbU7gaimgih8PqfUrQYI_89BsgqqKyie4bMtU8jhvS\""
      ],
      "metadata": {
        "id": "c8nVehOir_Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the process ID using port 8000\n",
        "!lsof -t -i:8000"
      ],
      "metadata": {
        "id": "F8zsvuPJuyFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill the process using the PID; repeat if multiple returned\n",
        "!kill -9 12345"
      ],
      "metadata": {
        "id": "vrdzUGVlu1qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 475"
      ],
      "metadata": {
        "id": "frmjHNAivDqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -t -i:8000"
      ],
      "metadata": {
        "id": "NYGgJkHpu5f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "id": "7FY2pYOGrJDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This project demonstrates the end-to-end development of a sentiment analysis web application using state-of-the-art NLP models, FastAPI for the backend, and a custom front-end served seamlessly in a Google Colab environment. By combining machine learning, API development, cloud tunneling with ngrok, and interactive user experience, the project showcases practical technical skills highly relevant for data analytics and modern data-driven product delivery.\n",
        "\n",
        "Beyond just building a working demo, this effort highlights experience in problem-solving, integrating multiple technologies, and delivering a polished, user-friendly application a valuable addition to any data analytics portfolio."
      ],
      "metadata": {
        "id": "K6x12W8Zn2yn"
      }
    }
  ]
}